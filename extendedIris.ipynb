{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "extendedIris.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patha325/AFAIGCG/blob/master/extendedIris.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx45mBh8oCcf",
        "colab_type": "text"
      },
      "source": [
        "Import the iris dataset, split data into X and classification into y, this time there are three types. check it using head() or tail() or other function! We will be using sklearn, please check the documentation for more indepth knowledge of the algorithm implementation. http://scikit-learn.org/stable/index.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvvGEttVoFv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, [2, 3]]\n",
        "y = iris.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bfVxu1OoIYo",
        "colab_type": "text"
      },
      "source": [
        "There is a predifined function to train our data into training and test sets, lets use it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNf56egwoJkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiFBIJnroKl3",
        "colab_type": "text"
      },
      "source": [
        "Good practice is to start by normalizing the data, look at X_train and X_train_std"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3ef5vVUoK69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "sc.fit(X_train)\n",
        "X_train_std = sc.transform(X_train)\n",
        "X_test_std = sc.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJRgFQXloNEh",
        "colab_type": "text"
      },
      "source": [
        "Initialize the sklearn perceptron, fit the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i48vyPsBoNhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "ppn = Perceptron(max_iter=40, eta0=0.1, random_state=0)\n",
        "ppn.fit(X_train_std, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMIaCSl9oP_A",
        "colab_type": "text"
      },
      "source": [
        "Using our test data, how many are missclassified?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30O_DIj9oRKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = ppn.predict(X_test_std)\n",
        "print('Missclassified samples: %d' % (y_test != y_pred).sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mJdisHxoSB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OtgXVpHohst",
        "colab_type": "text"
      },
      "source": [
        "Similar code as before, plot the regions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxQmxfzIoeCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_decision_regions(X, y, classifier,\n",
        "                       test_idx=None, resolution=0.02):\n",
        "    # setup marker generator and color map\n",
        "    markers = ('s', 'x', 'o', '^', 'v')\n",
        "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "    # plot the decision surface\n",
        "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "                            np.arange(x2_min, x2_max, resolution))\n",
        "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "    Z = Z.reshape(xx1.shape)\n",
        "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
        "    plt.xlim(xx1.min(), xx1.max())\n",
        "    plt.ylim(xx2.min(), xx2.max())\n",
        "    # plot all samples\n",
        "    X_test, y_test = X[test_idx, :], y[test_idx]\n",
        "    for idx, cl in enumerate(np.unique(y)):\n",
        "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
        "                       alpha=0.8, c=cmap(idx),\n",
        "                       marker=markers[idx], label=cl)\n",
        "    # highlight test samples\n",
        "    if test_idx:\n",
        "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
        "        plt.scatter(X_test[:, 0], X_test[:, 1], c='',\n",
        "                   alpha=1.0, linewidth=1, marker='o',\n",
        "                   s=55, label='test set')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZZzyxw9okYM",
        "colab_type": "text"
      },
      "source": [
        "Combine the test data and the train data to plot it easily."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fIoKxPiojRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_combined_std = np.vstack((X_train_std, X_test_std))\n",
        "y_combined = np.hstack((y_train, y_test))\n",
        "plot_decision_regions(X=X_combined_std,y=y_combined,classifier=ppn,test_idx=range(105,150))\n",
        "plt.xlabel('petal length [standardized]')\n",
        "plt.ylabel('petal width [standardized]')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXxoC2yLosZ7",
        "colab_type": "text"
      },
      "source": [
        "Lets do the same thing with a LogisticRegression instead of a perceptron. https://en.wikipedia.org/wiki/Logistic_regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EceRyCNomqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(C=1000.0, random_state=0)\n",
        "lr.fit(X_train_std, y_train)\n",
        "plot_decision_regions(X_combined_std,y_combined, classifier=lr,test_idx=range(105,150))\n",
        "plt.xlabel('petal length [standardized]')\n",
        "plt.ylabel('petal width [standardized]')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        "print('Accuracy: %.2f' % accuracy_score(y_test, lr.predict(X_test_std)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT3UsI0ho4s0",
        "colab_type": "text"
      },
      "source": [
        "Given the value of C, do we weight the parameters differently in our algorithm?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq-s4n-0o2gj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights, params = [], []\n",
        "for c in np.arange(-5, 5):\n",
        "    lr = LogisticRegression(C=10.0**c, random_state=0)\n",
        "    lr.fit(X_train_std, y_train)\n",
        "    weights.append(lr.coef_[1])\n",
        "    params.append(10.0**c)\n",
        "weights = np.array(weights)\n",
        "plt.plot(params, weights[:, 0],label='petal length')\n",
        "plt.plot(params, weights[:, 1], linestyle='--',label='petal width')\n",
        "plt.ylabel('weight coefficient')\n",
        "plt.xlabel('C')\n",
        "plt.legend(loc='upper left')\n",
        "plt.xscale('log')\n",
        "plt.show()\n",
        "print('Accuracy: %.2f' % accuracy_score(y_test, lr.predict(X_test_std)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2CtOi8Wo-dm",
        "colab_type": "text"
      },
      "source": [
        "Lets do the same thing with a SVC, using a linear kernel. instead of a perceptron, https://en.wikipedia.org/wiki/Support_vector_machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yvGXbjao58E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "svm = SVC(kernel='linear', C=1.0, random_state=0)\n",
        "svm.fit(X_train_std, y_train)\n",
        "plot_decision_regions(X_combined_std,y_combined, classifier=svm,test_idx=range(105,150))\n",
        "plt.xlabel('petal length [standardized]')\n",
        "plt.ylabel('petal width [standardized]')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        "print('Accuracy: %.2f' % accuracy_score(y_test, svm.predict(X_test_std)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKRMhtZ6pC70",
        "colab_type": "text"
      },
      "source": [
        "Change the kernal to rbf. (Radial basis function)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOA9UtCIpAks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svm = SVC(kernel='rbf', random_state=0, gamma=0.2, C=1.0)\n",
        "svm.fit(X_train_std, y_train)\n",
        "plot_decision_regions(X_combined_std,y_combined, classifier=svm,test_idx=range(105,150))\n",
        "plt.xlabel('petal length [standardized]')\n",
        "plt.ylabel('petal width [standardized]')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        "print('Accuracy: %.2f' % accuracy_score(y_test, svm.predict(X_test_std)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhGK5TAIpa8P",
        "colab_type": "text"
      },
      "source": [
        "How does the parameter gamma affect our model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSyx1SKbpHs8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svm = SVC(kernel='rbf', random_state=0, gamma=100.0, C=1.0)\n",
        "svm.fit(X_train_std, y_train)\n",
        "plot_decision_regions(X_combined_std,y_combined, classifier=svm,test_idx=range(105,150))\n",
        "plt.xlabel('petal length [standardized]')\n",
        "plt.ylabel('petal width [standardized]')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        "print('Accuracy: %.2f' % accuracy_score(y_test, svm.predict(X_test_std)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF_xzMbJpgAC",
        "colab_type": "text"
      },
      "source": [
        "Use a decisiontree, what regions does that produce? https://en.wikipedia.org/wiki/Decision_tree_learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsp03gqYpc3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "tree = DecisionTreeClassifier(criterion='entropy',max_depth=3, random_state=0)\n",
        "tree.fit(X_train, y_train)\n",
        "X_combined = np.vstack((X_train, X_test))\n",
        "y_combined = np.hstack((y_train, y_test))\n",
        "plot_decision_regions(X_combined, y_combined,classifier=tree, test_idx=range(105,150))\n",
        "plt.xlabel('petal length [cm]')\n",
        "plt.ylabel('petal width [cm]')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        "print('Accuracy: %.2f' % accuracy_score(y_test, tree.predict(X_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8qVVupUqThK",
        "colab_type": "text"
      },
      "source": [
        "Use a RandomForestClassifier, what regions does that produce? https://en.wikipedia.org/wiki/Random_forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vijmKrhzphIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "forest = RandomForestClassifier(criterion='entropy',n_estimators=10,random_state=1,n_jobs=2)\n",
        "forest.fit(X_train, y_train)\n",
        "plot_decision_regions(X_combined, y_combined,classifier=forest, test_idx=range(105,150))\n",
        "plt.xlabel('petal length')\n",
        "plt.ylabel('petal width')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        "print('Accuracy: %.2f' % accuracy_score(y_test, forest.predict(X_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd6oKPE9qeRe",
        "colab_type": "text"
      },
      "source": [
        "Same thing using KNN, https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhNMyuufqbGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5, p=2,metric='minkowski')\n",
        "knn.fit(X_train_std, y_train)\n",
        "plot_decision_regions(X_combined_std, y_combined,classifier=knn, test_idx=range(105,150))\n",
        "plt.xlabel('petal length [standardized]')\n",
        "plt.ylabel('petal width [standardized]')\n",
        "plt.show()\n",
        "print('Accuracy: %.2f' % accuracy_score(y_test, knn.predict(X_test_std)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}